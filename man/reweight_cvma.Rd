% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/reweight_cvma.R
\name{reweight_cvma}
\alias{reweight_cvma}
\title{Changing options for super learner and outcome weighting schemes after a call
to \code{cvma}.}
\usage{
reweight_cvma(object, Y, X, sl_control = list(ensemble_fn = "ensemble_linear",
  optim_risk_fn = "optim_risk_sl_se", weight_fn = "weight_sl_convex", cv_risk_fn
  = "cv_risk_sl_r2", family = gaussian(), alpha = 0.05),
  y_weight_control = list(ensemble_fn = "ensemble_linear", weight_fn =
  "weight_y_convex", optim_risk_fn = "optim_risk_y_r2", cv_risk_fn =
  "cv_risk_y_r2", alpha = 0.05), return_control = list(outer_weight = TRUE,
  outer_sl = TRUE, all_y = TRUE, all_learner_assoc = TRUE, all_learner_fits =
  FALSE), scale = FALSE)
}
\arguments{
\item{object}{An object of class \code{cvma}.}

\item{Y}{A matrix or data.frame of outcomes. This is assumed to be the same matrix
or data.frame of outcomes used in the original call to \code{cvma} and the function
will not check whether this is true. It is the responsibility of the user to ensure that
this function is invoked with the correct data set.}

\item{X}{A matrix or data.frame of predictors. This is assumed to be the same matrix
or data.frame of outcomes used in the original call to \code{cvma} and the function
will not check whether this is true. It is the responsibility of the user to ensure that
this function is invoked with the correct data set.}

\item{sl_control}{A list with named entries ensemble_fn, optim_risk_fn, weight_fn,
cv_risk_fn, family. Available functions can be viewed with \code{sl_control_options()}. See
\code{?sl_control_options} for more on how users may supply their own functions.}

\item{y_weight_control}{A list with named entries ensemble_fn, optim_risk_fn, weight_fn,
cv_risk_fn. Available functions can be viewed with \code{y_weight_control_options()}. See
\code{?y_weight_control_options} for more on how users may supply their own functions.}

\item{return_control}{A list with named entries \code{outer_weight} (whether to return outcome
weights for outer-most fold of CV, default \code{TRUE}), \code{outer_sl} (whether to return the 
super learner fit for each outcome on all the data), \code{all_y} (whether to return cross-validated
performance metrics for all outcomes), \code{all_learner_assoc} (whether to return cross-validation
performance metrics for all learners), \code{all_learner_fits} (whether to return all learner fits, 
which, while memory intensive, can be helpful if association measures based on different outcome 
weighting schemes are desired).}

\item{scale}{Standardize each outcome to be mean zero with standard deviation 1. This is assumed 
to be the same value as in the original call to \code{cvma} and the function
will not check whether this is true. It is the responsibility of the user to ensure that
this function is invoked with the correct option to match the original call.}
}
\value{
If \code{return_outer_sl} is TRUE, it will return for each outcome Super Learner fit weights 
and associated risk for each learner. In addition, it will return the fit for all learners based on 
all folds. If \code{return_outer_weight} is TRUE, it will return the weights for each outcome
obtained using V-1 cross-validation. If \code{return_all_y} is TRUE, it will return for each 
outcome cross-validated measure (nonparametric R-squared or AUC), confidence interval and associated
p-value. 

TO DO: Re-write this documentation. Ivana
}
\description{
This function allows you to use the learner fits from a previous call to \code{cvma}
to change options for how the super learner weights and outcome weights are computed. 
The majority of the computation time in \code{cvma} is spent fitting learners, while 
the (re-)computation of weights is relatively quick. Thus, this function allows one
to obtain new results without the need to re-fit a trove of learners.
}
\examples{
set.seed(1234)
library(SuperLearner)
library(future)
X <- data.frame(x1=runif(n=100,0,5), x2=runif(n=100,0,5))
Y1 <- rnorm(100, X$x1 + X$x2, 1)
Y2 <- rnorm(100, X$x1 + X$x2, 3)
Y <- data.frame(Y1 = Y1, Y2 = Y2)
# results for super learner and R^2 for convex 
# combination of outcomes
fit <- cvma(Y = Y, X = X, V = 5, 
            learners = c("SL.glm","SL.mean"), 
			   return_control = list(outer_weight = TRUE,
                                  outer_sl = TRUE,
                                  all_y = TRUE,
                                  all_learner_assoc = TRUE,
                                  all_learner_fits = TRUE))

# re-weight with discrete super learner and R^2 
# for a single outcome
re_fit <- reweight_cvma(fit, Y = Y, X = X, 
				sl_control = list(ensemble_fn = "ensemble_linear",
                               optim_risk_fn = "optim_risk_sl_se",
                               weight_fn = "weight_sl_01",
                               cv_risk_fn = "cv_risk_sl_r2",
                               family = gaussian(),
                               alpha = 0.05),
				y_weight_control = list(ensemble_fn = "ensemble_linear",
                                 weight_fn = "weight_y_01",
                                 optim_risk_fn = "optim_risk_y_r2",
                                 cv_risk_fn = "cv_risk_y_r2",
                                 alpha = 0.05))
}
\seealso{
predict method
}
