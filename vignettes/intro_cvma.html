<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="David Benkeser and Ivana Malenica" />

<meta name="date" content="2017-10-28" />

<title>A Machine learning-based approach for testing associations with multivariate outcomes</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">A Machine learning-based approach for testing associations with multivariate outcomes</h1>
<h4 class="author"><em>David Benkeser and Ivana Malenica</em></h4>
<h4 class="date"><em>2017-10-28</em></h4>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The <code>cvma</code> package provides a method for summarizing the strength of association between a set of variables and a multivariate outcome. In particular, <code>cvma</code> uses an ensemble machine learning-based approach to detect and quantify the measure of association of nonlinear relationships and covariate interactions. In addition, it allows for testing the strong null hypothesis of no association between a set of variables and a multivariate outcome. Lastly, <code>cvma</code> performs variable importance analysis, and therefore summarizes each groups’ association with the outcome.</p>
<hr />
</div>
<div id="installing-and-loading-the-package" class="section level2">
<h2>Installing and loading the package</h2>
<p>In the following sections, we examine the use of <code>cvma</code> in a variety of simple examples. The package can be installed as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span> (<span class="op">!</span>(<span class="st">&quot;cvma&quot;</span> <span class="op">%in%</span><span class="st"> </span><span class="kw">installed.packages</span>())) {
  devtools<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;benkeser/cvma&quot;</span>)
}</code></pre></div>
<p>Once the package is installed, we can load it using the following command:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">suppressMessages</span>(<span class="kw">library</span>(cvma))</code></pre></div>
<hr />
</div>
<div id="general-methodology" class="section level2">
<h2>General methodology</h2>
<p>The <code>cvma</code> package provides flexible interface for computing cross-validation-based measures of maximal association. The method implemented uses covariate information to estimate combination of outcomes that optimizes a measure of predictive performance for given prediction algorithms. In essence, it estimates the combination of outcomes that is “easiest to predict” based on the available covariates.</p>
<p>In an outer layer of V-fold cross validation, training samples are used to train a prediction algorithm for each outcome. Multiple algorithms may be used and their predictions combined using the Super Learner methodology <span class="citation">(van der Laan, Polley, and Hubbard 2007)</span>. Note that Super Learner weights will be estimated based on V-2 fold cross-validation.</p>
<p>An inner layer of V-1 cross validation is used to determine a user-specified combination of outcomes that maximizes a user-specified prediction criteria. The outer layer validation sample is used to compute a user-specified cross-validated measure of performance of the prediction algorithm for predicting the combined outcome that was computed in the training sample. Several common choices for outcome combinations (convex combination of outcomes and single outcome that is most associated) and prediction criteria (nonparametric R^2, negative log-likelihood, and area under ROC curve) are implemented. Note that users may specify their own criteria as well. All implemented control options for <code>cvma</code> can be seen using the <code>list_control_options</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">list_control_options</span>()</code></pre></div>
<pre><code>## All ensemble functions in cvma:</code></pre>
<pre><code>## [1] &quot;ensemble_linear&quot;       &quot;ensemble_logit_linear&quot;</code></pre>
<pre><code>## All weight functions used to select weighted super learner in cvma:</code></pre>
<pre><code>## [1] &quot;weight_sl_01&quot;     &quot;weight_sl_convex&quot;</code></pre>
<pre><code>## All weight functions used to select weighted outcomes in cvma:</code></pre>
<pre><code>## [1] &quot;weight_y_01&quot;     &quot;weight_y_convex&quot;</code></pre>
<pre><code>## All risk functions used to select weighted super learner in cvma:</code></pre>
<pre><code>## [1] &quot;optim_risk_sl_auc&quot;     &quot;optim_risk_sl_nloglik&quot; &quot;optim_risk_sl_se&quot;</code></pre>
<pre><code>## All risk functions used to select weighted outcomes in cvma:</code></pre>
<pre><code>## [1] &quot;optim_risk_y_auc&quot;     &quot;optim_risk_y_nloglik&quot; &quot;optim_risk_y_r2&quot;</code></pre>
<pre><code>## All cv-risk functions used to evaluate super learner predictions of single outcomes in cvma:</code></pre>
<pre><code>## [1] &quot;cv_risk_sl_auc&quot;     &quot;cv_risk_sl_nloglik&quot; &quot;cv_risk_sl_r2&quot;</code></pre>
<pre><code>## All cv-risk functions used to evaluate super learner predictions of combined outcomes in cvma:</code></pre>
<pre><code>## [1] &quot;cv_risk_y_auc&quot;     &quot;cv_risk_y_nloglik&quot; &quot;cv_risk_y_r2&quot;</code></pre>
<p>The function returns the cross-validated summary measure for the maximally combined outcome and, if desired, the cross-validated summary measure for each outcome.</p>
</div>
<div id="nonparametric-r2" class="section level2">
<h2>Nonparametric <span class="math inline">\(R^2\)</span></h2>
<p>In order to illustrate cross-validated nonparametric R-squared for evaluating maximum association, we first simulate a simple dataset with continuous outcomes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
X &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x1=</span><span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">5</span>), <span class="dt">x2=</span><span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">5</span>))
Y1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, X<span class="op">$</span>x1 <span class="op">+</span><span class="st"> </span>X<span class="op">$</span>x2, <span class="dv">1</span>)
Y2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, X<span class="op">$</span>x1 <span class="op">+</span><span class="st"> </span>X<span class="op">$</span>x2, <span class="dv">3</span>)
Y &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Y1 =</span> Y1, <span class="dt">Y2 =</span> Y2)</code></pre></div>
<p>Next, we run <code>cvma</code> with 10 folds and 3 learners (generalized linear models, mean and neural networks). In order to specify more algorithms, see <code>SuperLearner::listWrappers()</code> for implemented learners supported by the package <code>SuperLearner</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">cvma</span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">V =</span> <span class="dv">10</span>, <span class="dt">learners =</span> <span class="kw">c</span>(<span class="st">&quot;SL.glm&quot;</span>,<span class="st">&quot;SL.mean&quot;</span>,<span class="st">&quot;SL.nnet&quot;</span>))</code></pre></div>
<pre><code>## Loading required package: nnet</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit</code></pre></div>
<pre><code>##   cv_measure   ci_low   ci_high      p_value
## 1  0.7674728 0.665341 0.8384359 2.040982e-15</code></pre>
<p>We can explore the results of <code>cvma</code> a bit more.</p>
<ul>
<li><p><code>fit$cv_assoc</code> returns risk for the entire procedure, including the cross-validation measure, confidence interval, p-value and influence curve.</p></li>
<li><p><code>fit$cv_assoc_all_y</code> returns cross-validated performance metrics for all the outcomes, with combined learners.</p></li>
<li><p><code>fit$cv_assoc_all_learners</code> returns for each outcome and learner cross-validated metric, confidence interval, associated p-value and influence curve.</p></li>
<li><p><code>fit$sl_fits</code> will return the super learner fit for each outcome on all the data, including risk for each learner and their separate fits.</p></li>
<li><p><code>fit$outer_weight</code> returns outcome weights for outer-most fold of cross-validation and the mean of the combined outcomes.</p></li>
<li><p><code>fit$outer_weight</code> returns outcome weights for inner-most fold of cross-validation, folds used for training and the mean of the combined outcomes.</p></li>
</ul>
</div>
<div id="negative-log-likelihood" class="section level2">
<h2>Negative log-likelihood</h2>
<p>Instead of nonparametric <span class="math inline">\(R^2\)</span>, we can use negative log-likelihood as prediction criteria instead. Similarly as before, we first simulate a simple dataset with binary outcomes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)

X &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x1=</span><span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">5</span>), <span class="dt">x2=</span><span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">5</span>))
Y1 &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">100</span>, <span class="dv">1</span>, <span class="kw">plogis</span>(<span class="op">-</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span><span class="op">*</span>X<span class="op">$</span>x1 <span class="op">+</span><span class="st"> </span><span class="fl">0.2</span><span class="op">*</span>X<span class="op">$</span>x2))
Y2 &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">100</span>, <span class="dv">1</span>, <span class="kw">plogis</span>(<span class="op">-</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span><span class="op">*</span>X<span class="op">$</span>x1))
Y &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Y1 =</span> Y1, <span class="dt">Y2 =</span> Y2)</code></pre></div>
<p>We run <code>cvma</code> with 10 folds and 3 learners as before, but this time specifying the <code>cv_risk_fn</code> for both <code>sl_control</code> and <code>y_weight_control</code> to negative log-likelihood cross-validated measure. We also define loss by the negative log-likelihood, and change the outcome type in to binomial() for prediction.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">cvma</span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">V =</span> <span class="dv">10</span>, 
              <span class="dt">learners =</span> <span class="kw">c</span>(<span class="st">&quot;SL.glm&quot;</span>,<span class="st">&quot;SL.mean&quot;</span>,<span class="st">&quot;SL.nnet&quot;</span>),
              <span class="dt">sl_control =</span> <span class="kw">list</span>(<span class="dt">ensemble_fn =</span> <span class="st">&quot;ensemble_linear&quot;</span>,
                                <span class="dt">optim_risk_fn =</span> <span class="st">&quot;optim_risk_sl_nloglik&quot;</span>,
                                <span class="dt">weight_fn =</span> <span class="st">&quot;weight_sl_convex&quot;</span>,
                                <span class="dt">cv_risk_fn =</span> <span class="st">&quot;cv_risk_sl_nloglik&quot;</span>,
                                <span class="dt">family =</span> <span class="kw">binomial</span>(),
                                <span class="dt">alpha =</span> <span class="fl">0.05</span>),
              <span class="dt">y_weight_control =</span> <span class="kw">list</span>(<span class="dt">ensemble_fn =</span> <span class="st">&quot;ensemble_linear&quot;</span>,
                                      <span class="dt">weight_fn =</span> <span class="st">&quot;weight_y_01&quot;</span>,
                                      <span class="dt">optim_risk_fn =</span> <span class="st">&quot;optim_risk_y_nloglik&quot;</span>,
                                      <span class="dt">cv_risk_fn =</span> <span class="st">&quot;cv_risk_y_nloglik&quot;</span>,
                                      <span class="dt">alpha =</span> <span class="fl">0.05</span>))

fit</code></pre></div>
<pre><code>##   cv_measure    ci_low   ci_high   p_value
## 1  0.6302028 0.4829439 0.7774617 0.2010806</code></pre>
</div>
<div id="area-under-roc-curve" class="section level2">
<h2>Area under ROC curve</h2>
<p>Finally, we can use the area area under ROC curve as prediction criteria. In order to accomplish this, we use auc appropriate <code>cv_risk_fn</code>, <code>optim_risk_fn</code> and <code>weight_fn</code> as illustrated below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">cvma</span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">V =</span> <span class="dv">5</span>, 
                <span class="dt">learners =</span> <span class="kw">c</span>(<span class="st">&quot;SL.glm&quot;</span>,<span class="st">&quot;SL.mean&quot;</span>),
                <span class="dt">sl_control =</span> <span class="kw">list</span>(<span class="dt">ensemble_fn =</span> <span class="st">&quot;ensemble_linear&quot;</span>,
                                   <span class="dt">optim_risk_fn =</span> <span class="st">&quot;optim_risk_sl_nloglik&quot;</span>,
                                   <span class="dt">weight_fn =</span> <span class="st">&quot;weight_sl_convex&quot;</span>,
                                   <span class="dt">cv_risk_fn =</span> <span class="st">&quot;cv_risk_sl_auc&quot;</span>,
                                   <span class="dt">family =</span> <span class="kw">binomial</span>(),
                                   <span class="dt">alpha =</span> <span class="fl">0.05</span>),
                <span class="dt">y_weight_control =</span> <span class="kw">list</span>(<span class="dt">ensemble_fn =</span> <span class="st">&quot;ensemble_linear&quot;</span>,
                                  <span class="dt">weight_fn =</span> <span class="st">&quot;weight_y_01&quot;</span>,
                                  <span class="dt">optim_risk_fn =</span> <span class="st">&quot;optim_risk_y_auc&quot;</span>,
                                  <span class="dt">cv_risk_fn =</span> <span class="st">&quot;cv_risk_y_auc&quot;</span>,
                                  <span class="dt">alpha =</span> <span class="fl">0.05</span>))
fit</code></pre></div>
<pre><code>##   cv_measure   ci_low   ci_high    p_value
## 1  0.6228493 0.481018 0.7646807 0.04478675</code></pre>
</div>
<div id="variable-importance" class="section level2">
<h2>Variable Importance</h2>
<p>Instead of assessing the overall summary of association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, one might be interested in quantifying the relative importance of each component of <span class="math inline">\(X\)</span>. The function <code>compare_cvma</code> function computes the differences in the estimated association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> when considering different sets of variables. In essence, the proposed method is similar to well known random forests, in that it measures the change in predictive performance with and without each predictor being considered <span class="citation">(Breiman 2001)</span>. We emphasize that the method implemented in <code>compare_cvma</code> yields straightforward, asymptotically justified inference, as opposed to many existing variable importance methods.</p>
<p>Once again, we simulate a simple data structure in order to illustrate how to use <code>compare_cvma</code> function to assess variable importance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)

X &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x1=</span><span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">5</span>), <span class="dt">x2=</span><span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">5</span>), <span class="dt">x3=</span><span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">5</span>))
Y1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, X<span class="op">$</span>x1 <span class="op">+</span><span class="st"> </span>X<span class="op">$</span>x2, <span class="dv">1</span>)
Y2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, X<span class="op">$</span>x1 <span class="op">+</span><span class="st"> </span>X<span class="op">$</span>x2 <span class="op">+</span><span class="st"> </span>X<span class="op">$</span>x3, <span class="dv">3</span>)
Y &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Y1 =</span> Y1, <span class="dt">Y2 =</span> Y2)</code></pre></div>
<p>For computational simplicity of this example, we only look at low-dimensional <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and fit with simple learners and <span class="math inline">\(V=10\)</span>. To assess the importance of variable <span class="math inline">\(x3\)</span> with <span class="math inline">\(x3 \in X\)</span>, we repeat the procedure described in the previous section but restrict to variables not in this subset. Therefore, we obtain an estimate of the cross-validated performance measure for the composite prediction algorithm based on a subset of variables, as well as when all variables are used. First we fit data with all variables in <span class="math inline">\(X\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit1 &lt;-<span class="st"> </span><span class="kw">cvma</span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">V =</span> <span class="dv">10</span>, <span class="dt">learners =</span> <span class="kw">c</span>(<span class="st">&quot;SL.glm&quot;</span>,<span class="st">&quot;SL.mean&quot;</span>))</code></pre></div>
<p>Then, we repeat the procedure restricting to variables not in the subset <span class="math inline">\(\{x3\}\)</span> of <span class="math inline">\(X\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit2 &lt;-<span class="st"> </span><span class="kw">cvma</span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X[,<span class="op">-</span><span class="dv">3</span>], <span class="dt">V =</span> <span class="dv">10</span>, <span class="dt">learners =</span> <span class="kw">c</span>(<span class="st">&quot;SL.glm&quot;</span>,<span class="st">&quot;SL.mean&quot;</span>))</code></pre></div>
<p>We are now ready to assess the variable importance of <span class="math inline">\(x3\)</span> for association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The package <code>cvma</code> provides 3 different options for the type of comparison to be made. With <span class="math inline">\(R^2\)</span> as the example metric, we can look at the differences in <span class="math inline">\(R^2\)</span> between the two fits using the following command:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit&lt;-<span class="kw">compare_cvma</span>(fit1, fit2, <span class="dt">contrast =</span> <span class="st">&quot;diff&quot;</span>)
fit</code></pre></div>
<pre><code>##      contrast     ci_low   ci_high  p_value
## 1 0.001624902 -0.1203579 0.1236077 0.979171</code></pre>
<p>The output of <code>compare_cvma</code> includes the contrast measure, associated confidence interval for <span class="math inline">\(1-\alpha\)</span>, and p-value. In addition, we can look at the ratio of <span class="math inline">\(R^2\)</span> instead, with symmetric CI.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit&lt;-<span class="kw">compare_cvma</span>(fit1, fit2, <span class="dt">contrast =</span> <span class="st">&quot;ratio&quot;</span>)
fit</code></pre></div>
<pre><code>##   contrast    ci_low  ci_high   p_value
## 1 1.002059 0.8472941 1.156825 0.9791933</code></pre>
<p>Finally, we can also assess variable importance using the ratio of <span class="math inline">\(R^2\)</span>, with symmetric CI on the log-scale:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit&lt;-<span class="kw">compare_cvma</span>(fit1, fit2, <span class="dt">contrast =</span> <span class="st">&quot;logratio&quot;</span>)
fit</code></pre></div>
<pre><code>##   contrast    ci_low  ci_high   p_value
## 1 1.002059 0.8585222 1.169595 0.9791925</code></pre>
</div>
<div id="session-information" class="section level2">
<h2>Session Information</h2>
<pre><code>## R version 3.4.2 (2017-09-28)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] nnet_7.3-12         cvma_0.1.0          SuperLearner_2.0-22
## [4] nnls_1.4           
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.13        knitr_1.17          magrittr_1.5       
##  [4] Rsolnp_1.16         stringr_1.2.0       ROCR_1.0-7         
##  [7] globals_0.10.3      caTools_1.17.1      tools_3.4.2        
## [10] parallel_3.4.2      data.table_1.10.4-2 KernSmooth_2.23-15 
## [13] htmltools_0.3.6     cvAUC_1.1.0         gtools_3.5.0       
## [16] yaml_2.1.14         rprojroot_1.2       digest_0.6.12      
## [19] codetools_0.2-15    bitops_1.0-6        evaluate_0.10.1    
## [22] rmarkdown_1.6       gdata_2.18.0        stringi_1.1.5      
## [25] compiler_3.4.2      gplots_3.0.1        backports_1.1.1    
## [28] future_1.6.2        truncnorm_1.0-7     listenv_0.6.0</code></pre>
<hr />
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-breiman2001">
<p>Breiman, Leo. 2001. “Random Forests.” <em>Machine Learning</em> 45 (1). Springer: 5–32.</p>
</div>
<div id="ref-vdl2007super">
<p>van der Laan, Mark J., Eric C. Polley, and Alan E. Hubbard. 2007. “Super Learner.” <em>Statistical Applications in Genetics and Molecular Biology</em> 6 (1).</p>
</div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
